{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "cwd = os.getcwd()\n",
    "dataset_dir = os.path.join(cwd,'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(os.path.join(dataset_dir,\"artists.txt\")) as file:\n",
    "    artists = [artist for artist in file.readlines()][0].split(\" \")\n",
    "\n",
    "dirs = os.listdir(dataset_dir)\n",
    "wanted_artists_dirs = [wanted_artist for wanted_artist in dirs if any(artist in wanted_artist.lower() for artist in artists)]\n",
    "wanted_artists_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "training_data = []\n",
    "count = 0\n",
    "\n",
    "for dir in wanted_artists_dirs:\n",
    "    complete_dir = os.path.join(dataset_dir,dir)\n",
    "\n",
    "    for filename in os.listdir(complete_dir):\n",
    "        path = os.path.join(complete_dir, filename)\n",
    "        image = Image.open(path).resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
    "        training_data.append(np.asarray(image))\n",
    "        count += 1\n",
    "\n",
    "print(f\"Images:\",count)       \n",
    "training_data = np.reshape(training_data, (-1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "training_data = training_data / 127.5 - 1\n",
    "\n",
    "np.save('data.npy', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "\n",
    "for dir in os.listdir(dataset_dir):\n",
    "    subdir = os.path.join(dataset_dir,dir)\n",
    "    for filename in os.listdir(subdir):\n",
    "        filenames.append(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50  import preprocess_input \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, filenames, preprocessing_function=preprocess_input, out_shape=[IMAGE_SIZE,IMAGE_SIZE],img_generator=ImageDataGenerator(rescale=1./255 - 0.5)):\n",
    "\n",
    "        self.filenames = filenames\n",
    "        self.preprocessing_function = preprocessing_function\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def __len__(self): return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Read Image\n",
    "        curr_filename = self.filenames[index]\n",
    "        img = Image.open(join(training_dir, str(curr_filename) + '.png')).convert('RGB')\n",
    "\n",
    "        # Resize image\n",
    "        resized_img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        img_arr = np.asarray(resized_img)\n",
    "\n",
    "        return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_dataset = CustomDataset(filenames=filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: (None, 128, 128, 3), types: tf.uint8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_custom_dataset,\n",
    "                                               output_types=(tf.uint8),\n",
    "                                               output_shapes=([IMAGE_SIZE,IMAGE_SIZE, 3]))\n",
    "train_dataset = train_dataset.batch(32)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Preview image Frame\n",
    "PREVIEW_ROWS = 4\n",
    "PREVIEW_COLS = 7\n",
    "PREVIEW_MARGIN = 4\n",
    "SAVE_FREQ = 100\n",
    "\n",
    "# Size vector to generate images from\n",
    "NOISE_SIZE = 100\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 10000 \n",
    "BATCH_SIZE = 32\n",
    "GENERATE_RES = 3\n",
    "IMAGE_SIZE = 128 # rows/cols\n",
    "IMAGE_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(image_shape):\n",
    "   \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "   \n",
    "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    input_image = Input(shape=image_shape)\n",
    "    validity = model(input_image)\n",
    "    \n",
    "    return Model(input_image, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(noise_size, channels):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(4 * 4 * 256, activation=\"relu\", input_dim=noise_size))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    for i in range(GENERATE_RES):\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "    model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "    input = Input(shape=(noise_size,))\n",
    "    generated_image = model(input)\n",
    "    \n",
    "    return Model(input, generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(cnt, noise):\n",
    "    \n",
    "    rows = PREVIEW_MARGIN + (PREVIEW_ROWS * (IMAGE_SIZE + PREVIEW_MARGIN))\n",
    "    columns = PREVIEW_MARGIN + (PREVIEW_COLS * (IMAGE_SIZE + PREVIEW_MARGIN))\n",
    "\n",
    "    image_array = np.full((rows,columns,3), 255, dtype=np.uint8)\n",
    "\n",
    "    # predict a noise\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = 0.5 * generated_images + 0.5\n",
    "    \n",
    "    image_count = 0\n",
    "    \n",
    "    for row in range(PREVIEW_ROWS):\n",
    "        for col in range(PREVIEW_COLS):\n",
    "            r = row * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN\n",
    "            c = col * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN\n",
    "            image_array[r:r + IMAGE_SIZE, c:c + IMAGE_SIZE] = generated_images[image_count] * 255\n",
    "            image_count += 1\n",
    "            \n",
    "    output_path = 'output'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    filename = os.path.join(output_path, f\"trained-{cnt}.png\")\n",
    "    im = Image.fromarray(image_array)\n",
    "    im.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(1.5e-4, 0.5)\n",
    "loss = \"binary_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "image_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 1)                 1720385   \n",
      "=================================================================\n",
      "Total params: 1,720,385\n",
      "Trainable params: 1,718,465\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator(image_shape)\n",
    "\n",
    "discriminator.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(discriminator, to_file='discr_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator(NOISE_SIZE, IMAGE_CHANNELS)\n",
    "generator.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = Input(shape=(NOISE_SIZE,))\n",
    "generated_image = generator(random_input)\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(generated_image)\n",
    "\n",
    "combined = Model(random_input, validity)\n",
    "combined.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = np.ones((BATCH_SIZE, 1))\n",
    "y_fake = np.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "fixed_noise = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, NOISE_SIZE))\n",
    "cnt = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    idx = np.random.randint(0, training_data.shape[0], BATCH_SIZE)\n",
    "\n",
    "    #taking a sample from a real image and putting that on x_real\n",
    "    x_real = training_data[idx]\n",
    "\n",
    "    # defining a noise vector\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, NOISE_SIZE))\n",
    "    # passing that to our generator model to generate a fake image in x_fake.\n",
    "    x_fake = generator.predict(noise)\n",
    "\n",
    "    # training our discriminator model in both real and fake images separately\n",
    "    discriminator_metric_real = discriminator.train_on_batch(x_real, y_real)\n",
    "    discriminator_metric_generated = discriminator.train_on_batch(x_fake, y_fake)\n",
    " \n",
    "    discriminator_metric = 0.5 * np.add(discriminator_metric_real, discriminator_metric_generated)\n",
    "\n",
    "    generator_metric = combined.train_on_batch(noise, y_real)\n",
    "\n",
    "if epoch % 5 == 0:\n",
    "\n",
    "    save_images(cnt, fixed_noise)\n",
    "    cnt += 1\n",
    "    print(f\"{epoch} epoch, Discriminator accuracy: {100*  discriminator_metric[1]}, Generator accuracy: {100 * generator_metric[1]}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
